{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817e3ca0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628559cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"]=\"1\"\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import toml\n",
    "from time import time\n",
    "\n",
    "from src.data_utils import color_augmentations\n",
    "from src.embedding_loss import SpatialEmbLoss\n",
    "from src.unet import UNet\n",
    "from src.spatial_augmenter import SpatialAugmenter\n",
    "from src.train_utils import instance_seg_train_step, instance_seg_validation, save_snapshot, save_model\n",
    "from src.data_utils import H5pyDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c06b65",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c9af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_dataloader, inst_loss_fn, device, step, writer, validation_loss, loss, inst_model):\n",
    "    print('Validate')\n",
    "    val_new = instance_seg_validation(model,\n",
    "                        validation_dataloader,\n",
    "                        inst_loss_fn,\n",
    "                        device,\n",
    "                        step,\n",
    "                        writer,\n",
    "                        inst_model)\n",
    "    validation_loss.append(val_new)\n",
    "    if val_new <= np.min(validation_loss):\n",
    "        print('Save best model')\n",
    "        save_model(step, model, optimizer, loss, os.path.join(log_dir,\"best_model\"))\n",
    "\n",
    "def create_snapshot(img_caug, gt_inst, params, pred_inst, inst_loss_fn, step):\n",
    "    print('Save snapshot')\n",
    "    tmp_dic = {\n",
    "            'img_caug': img_caug[0].squeeze(0).cpu().detach().numpy(),\n",
    "            'gt_inst': gt_inst.squeeze(0).cpu().detach().numpy()[:1]\n",
    "    }\n",
    "    if params['instance_seg'] == 'embedding':\n",
    "        _,_,h,w = pred_inst.shape\n",
    "        xym_s = inst_loss_fn.xym[:, 0:h, 0:w].contiguous()\n",
    "        spatial_emb = pred_inst[0, 0:2] + xym_s  # 2 x h x w\n",
    "        sigma = pred_inst[0, 2:2+inst_loss_fn.n_sigma]  # n_sigma x h x w\n",
    "        seed_map = torch.sigmoid(\n",
    "            pred_inst[0, 2+inst_loss_fn.n_sigma:2+inst_loss_fn.n_sigma + 1])  # 1 x h x w\n",
    "        tmp_dic['embedding'] = spatial_emb.cpu().detach().numpy()\n",
    "        tmp_dic['sigma'] = sigma.cpu().detach().numpy()\n",
    "        tmp_dic['seed_map'] = seed_map.cpu().detach().numpy()\n",
    "    elif params['instance_seg'] == 'cpv_3c':\n",
    "        tmp_dic['pred_cpv'] = pred_inst[0,:2].cpu().detach().numpy()\n",
    "        tmp_dic['pred_3c'] = pred_inst[0,2:].softmax(0).cpu().detach().numpy()\n",
    "    save_snapshot(snap_dir, tmp_dic, step)\n",
    "\n",
    "\n",
    "def supervised_training(params, model, labeled_dataloader, validation_dataloader, fast_aug, color_aug_fn, inst_loss_fn, writer, device):\n",
    "    # step = -1, step = 21000\n",
    "    step = -1\n",
    "    validation_loss = []\n",
    "\n",
    "    # training loop\n",
    "    while step < params['training_steps']:\n",
    "\n",
    "        # get sample for training\n",
    "        tmp_loader = iter(labeled_dataloader)\n",
    "        for raw, gt in tmp_loader:\n",
    "            step += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # training step\n",
    "            print(\"Started training step %d\" % step)\n",
    "            loss, pred_inst, img_caug, gt_inst = instance_seg_train_step(model,\n",
    "                                                                         raw,\n",
    "                                                                         gt,\n",
    "                                                                         fast_aug,\n",
    "                                                                         color_aug_fn,\n",
    "                                                                         inst_loss_fn,\n",
    "                                                                         writer,\n",
    "                                                                         device,\n",
    "                                                                         step)\n",
    "            if torch.isnan(loss) or not torch.isfinite(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            print(\"backward loss done\")\n",
    "            optimizer.step()\n",
    "            print(\"Finished training step %d\" % step)\n",
    "\n",
    "            # validation\n",
    "            if step % params['validation_step'] == 0:\n",
    "                validate(model,\n",
    "                         validation_dataloader,\n",
    "                         inst_loss_fn,\n",
    "                         device,\n",
    "                         step,\n",
    "                         writer,\n",
    "                         validation_loss,\n",
    "                         loss,\n",
    "                         inst_model=params['instance_seg']\n",
    "                         )\n",
    "\n",
    "            # Create snapshot\n",
    "            if step % params['snapshot_step'] == 0:\n",
    "                create_snapshot(img_caug, gt_inst, params, pred_inst, inst_loss_fn, step)\n",
    "\n",
    "            # Create checkpoint\n",
    "            if step % params['checkpoint_step'] == 0:\n",
    "                save_model(step, model, optimizer, loss, os.path.join(log_dir,\"checkpoint_step_\"+str(step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1, 2, 1, 2])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2)\n",
    "x.size()\n",
    "y = torch.squeeze(x, 0)\n",
    "y.size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "70a592ef",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ead590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-19 16:28:47.304692: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-19 16:28:49.203192: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/julia/.local/share/virtualenvs/tau_nerve_segmentation-s0uDq-Et/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-19 16:28:49.203281: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/julia/.local/share/virtualenvs/tau_nerve_segmentation-s0uDq-Et/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-01-19 16:28:49.203291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directories set\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "params = {\n",
    "    'data_path': '/home/julia/AG KainmÃ¼ller/Alzheimer_Segmentation_Annotation/tau_nerve_segmentation/data/data/cells',\n",
    "    # directory name of report\n",
    "    'experiment' : 'instance_seg_cell_bodies',\n",
    "    'batch_size': 1,\n",
    "    'training_steps': 10, #:400000,\n",
    "    'in_channels': 3,\n",
    "    'num_fmaps': 32,\n",
    "    'fmap_inc_factors': 2,\n",
    "    'downsample_factors': [ [ 2, 2,], [ 2, 2,], [ 2, 2,], [ 2, 2,],],\n",
    "    'num_fmaps_out': 5,\n",
    "    'constant_upsample': False,\n",
    "    'padding': 'same',\n",
    "    'activation': 'ReLU',\n",
    "    'weight_decay': 1e-5,\n",
    "    'learning_rate': 3e-4,\n",
    "    'seed': 42,\n",
    "    'num_validation': 15,\n",
    "    'checkpoint_path': None, # 'exp_0_dsb/best_model',\n",
    "    'pretrained_model': False,\n",
    "    'multi_head': False,\n",
    "    'uniform_class_sampling': False,\n",
    "    'optimizer': 'AdamW', # one of SGD AdamW AdaBound , Adahessian breaks memory and is not supported\n",
    "    'validation_step' : 10, #500,\n",
    "    'snapshot_step' : 5, #5000,\n",
    "    'checkpoint_step': 5, #20000,\n",
    "    'instance_seg': 'embedding', # 'embedding' or 'cpv_3c'\n",
    "    'attention': True,\n",
    "    'color_augmentation_s': 0.4,\n",
    "    'to_center': True\n",
    "    }\n",
    "\n",
    "# augmentation parameters\n",
    "aug_params_fast = {}\n",
    "\"\"\"\n",
    "\n",
    "    'mirror': {'prob_x': 0.5, 'prob_y': 0.5, 'prob': 0.5},\n",
    "    'translate': {'max_percent':0.05, 'prob': 0.2},\n",
    "    'scale': {'min': 0.8, 'max':1.2, 'prob': 0.2},\n",
    "    'zoom': {'min': 0.8, 'max':1.2, 'prob': 0.2},\n",
    "    'rotate': {'max_degree': 179, 'prob': 0.75},\n",
    "    'shear': {'max_percent': 0.1, 'prob': 0.2},\n",
    "    'elastic': {'alpha': [120,120], 'sigma': 8, 'prob': 0.5}\n",
    "}\"\"\"\n",
    "print(\"parameters set\")\n",
    "\n",
    "# set directories for report\n",
    "log_dir = os.path.join(params['experiment'],'train')\n",
    "snap_dir = os.path.join(log_dir,'snaps')\n",
    "os.makedirs(snap_dir, exist_ok=True)\n",
    "writer_dir = os.path.join(log_dir,'summary', str(time()))\n",
    "os.makedirs(writer_dir, exist_ok=True)\n",
    "writer = SummaryWriter(writer_dir)\n",
    "print(\"directories set\")\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "params['device'] = device\n",
    "params['aug_params_fast'] = aug_params_fast\n",
    "with open(os.path.join(params['experiment'], 'params.toml'), 'w') as f:\n",
    "    toml.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b73ed0",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d8ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [os.path.join(params['data_path'], file) for file in os.listdir(params['data_path'])]\n",
    "random.Random(params['seed']).shuffle(file_paths)\n",
    "val_paths = [file_paths.pop(0) for _ in range(params['num_validation'])]\n",
    "labeled_paths = file_paths\n",
    "labeled_dataset = H5pyDataset(labeled_paths, raw_keys=['raw'], label_keys=['gt_instances'], crop_size=(1200, 1312))\n",
    "validation_dataset = H5pyDataset(val_paths, raw_keys=['raw'], label_keys=['gt_instances'], crop_size=(1200, 1312))\n",
    "\n",
    "labeled_dataloader = DataLoader(labeled_dataset,\n",
    "                        batch_size=params['batch_size'],\n",
    "                        shuffle=True,\n",
    "                        #prefetch_factor=4,\n",
    "                        num_workers=0)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_dataset,\n",
    "                    batch_size=1,\n",
    "                    shuffle=True,\n",
    "                   # prefetch_factor=4,\n",
    "                    num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efe691",
   "metadata": {},
   "source": [
    "### Select model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "452ee05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels = params['in_channels'],\n",
    "                num_fmaps = params['num_fmaps'],\n",
    "                fmap_inc_factor = params['fmap_inc_factors'],\n",
    "                downsample_factors = params['downsample_factors'],\n",
    "                activation = params['activation'],\n",
    "                padding = params['padding'],\n",
    "                num_fmaps_out = params['num_fmaps_out'],\n",
    "                constant_upsample = params['constant_upsample'],\n",
    "            ).to(params['device'])\n",
    "\n",
    "if 'checkpoint_path' in params.keys() and params['checkpoint_path']:\n",
    "    model.load_state_dict(torch.load(params['checkpoint_path'])['model_state_dict'])\n",
    "\n",
    "model = model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9b3b8",
   "metadata": {},
   "source": [
    "### Initialize optimizer and augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1977d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created spatial emb loss function with: to_center: True, n_sigma: 1, foreground_weight: 10\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "if params['optimizer'] == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                lr=params['learning_rate'],\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=params['weight_decay'],\n",
    "                                nesterov=True)\n",
    "elif params['optimizer'] == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=params['learning_rate'],\n",
    "                                  weight_decay=params['weight_decay'])\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=params['training_steps'], eta_min=5e-6)\n",
    "\n",
    "# Augmentation\n",
    "fast_aug = SpatialAugmenter(aug_params_fast)#, padding_mode='reflection')\n",
    "color_aug_fn = color_augmentations(100, s=params['color_augmentation_s'])\n",
    "\n",
    "# set loss function\n",
    "inst_loss_fn = SpatialEmbLoss(n_sigma=1, to_center=params['to_center'], foreground_weight=10, H=1200, W=1312).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547e968",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62280a6",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training step 0\n",
      "step started\n",
      "finished aug\n",
      "loss 2.4153785705566406\n",
      "backward loss done\n",
      "Finished training step 0\n",
      "Validate\n",
      "Validation loss:  2.3075337409973145\n",
      "Save best model\n",
      "Save model\n",
      "Save snapshot\n",
      "Save training snapshot\n",
      "Save model\n",
      "Started training step 1\n",
      "step started\n",
      "finished aug\n",
      "loss 2.3503928184509277\n",
      "backward loss done\n",
      "Finished training step 1\n",
      "Started training step 2\n",
      "step started\n",
      "finished aug\n",
      "loss 2.431063175201416\n",
      "backward loss done\n",
      "Finished training step 2\n",
      "Started training step 3\n",
      "step started\n",
      "finished aug\n"
     ]
    }
   ],
   "source": [
    "supervised_training(params, model, labeled_dataloader, validation_dataloader, fast_aug, color_aug_fn, inst_loss_fn, writer, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
